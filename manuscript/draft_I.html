<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Fifty Years of Memory Errors: A Meta-Analytic Review of the Misinformation Effect</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="draft_I_files/libs/clipboard/clipboard.min.js"></script>
<script src="draft_I_files/libs/quarto-html/quarto.js"></script>
<script src="draft_I_files/libs/quarto-html/popper.min.js"></script>
<script src="draft_I_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="draft_I_files/libs/quarto-html/anchor.min.js"></script>
<link href="draft_I_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="draft_I_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="draft_I_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="draft_I_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="draft_I_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Fifty Years of Memory Errors: A Meta-Analytic Review of the Misinformation Effect</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Misinformation may come in different shapes and be imposed via various means and channels. Here, we define misinformation as information presented after an event that is not congruent with what objectively transpired, regardless of whether it was intentionally deceptive. This phenomenon poses a challenge because our memories can readily incorporate coherent and seemingly reliable misinformation that aligns with our existing beliefs (Lewandowsky et al., 2012). This characteristic is arguably a feature rather than an error inherent in human communication and information exchange (Gilbert, 1991), although it can lead to severe consequences (Berkowitz &amp; Loftus, 2018). Another issue associated with the presence of misinformation is that humans generally struggle to detect it. Even in cases where misinformation is recognized, our recollection may still be influenced by it (Walter &amp; Murphey, 2017). This thesis scrutinizes the impact of misinformation on memory accuracy by reviewing the vast literature of the phenomenon known as the misinformation effect. While there already exist meta-analyses on the subject, all of them cover subsets of the literature addressing specific issues such as the moderating effects of warnings, corrections, or age. In this thesis, we aim to compile and synthesize the misinformation effect literature in its entirety, providing a broad overview of the effect and how it varies across experimental conditions.</p>
<section id="the-misinformation-effect-paradigm" class="level2">
<h2 class="anchored" data-anchor-id="the-misinformation-effect-paradigm">The Misinformation Effect Paradigm</h2>
<p>Viewing memory as malleable and unreliable was controversial among scholars in the early 1970s (Wixted et al., 2018). This notion was challenged as a study that seemingly had induced false memories in their subjects was published. The study was conducted by Elizabeth Loftus (1975), where participants were presented with a videotape displaying an automobile accident. When later asked about the accident, several participants reported having seen a barn that was never depicted in the videotape. The explanation behind this finding was that half of the participants were exposed to false information after viewing the accident. When asked about the accident one week later, it was clear that the memory of the event differed between the group who had received misinformation, and the group who had not. This finding laid the base for what is now referred to as the “misinformation effect”. This effect refers to the reduced recall accuracy in individuals that have been exposed to misleading information, compared to non-misled individuals. Additionally, the experiment by Loftus led to what is commonly described as the three-phase misinformation effect paradigm. The paradigm comprises three phases following a set order: (1) an event phase, followed by (2) a post-event information (PEI) phase, and lastly (3) a test phase. In the first phase, participants experience some type of to-be-remembered event or information. In the second phase, the participants are exposed to some type of PEI about the previous event or information. In the last phase, the participants recall the event or information that they were exposed to in the first phase. Depending on the design of the experiment, participants receive either misleading and/or control information in the post-event phase. The potential difference in memory accuracy between misled and control items/conditions in the final test phase is what constitutes the misinformation effect.</p>
<p>To explain these memory errors, Loftus (1975) hypothesized that when people are exposed to an item or detail that was absent during the original event, the missing item or detail is added to the mental representation of the experienced event. When later questioned about the event, people base their response on the reconstruction and supplementation of the original event and through this reconstruction, people may perceive and visualize an item that was never present to begin with.</p>
<p>Ayers and Reder (1998) described theories explaining memory errors based on impairment of the original memory as “destructive updating” or “trace alteration”. When the misleading information is consolidated into long-term memory, it replaces contradictory information in the initial memory. This places the causal root of the misinformation effect at the introduction of misinformation rather than at the initial encoding or the retrieval of the memory. Memories are encoded and retrieved correctly, just distorted by additional information. Another account for why these memory errors occur, is that misinformation may serve as a blocker for the initial memory trace, which in turn causes memory impairment. This explanation postulates that two alternative traces for the same information exist and the recall depends on the comparative accessibility of the traces where the most recent trace is recalled (Bekerian &amp; Bowers, 1983). To put the explanation of Bekerian &amp; Bowers (1983) to the test, Lindsay &amp; Johnson (1989b) conducted a study where they introduced misinformation prior to witnessing the original event. They found that exposure to misinformation prior to witnessing an event also caused a misinformation effect. This provided evidence against the trace blocking theory through a phenomenon known as the eversed eyewitness suggestibility effect.</p>
<p>In 1985, McCloskey and Zaragoza challenged the hypothesis of memory impairment provided by Loftus (1975), arguing that a memory is not “overwritten” by the integration of a second memory. The memory is merely forgotten or non-retrievable. Accordingly, the misinformation effect occurs when a participant fails to recollect the original stimuli for reasons other than the introduction of misinformation. McCloskey and Zaragoza (1985) developed a modified test procedure to examine their hypothesis, where the misled item was not present on the final test, only the original event item and a novel item. Their findings suggested that error in event encoding provided a better explanation for the misinformation effect than error in retrieval. The modified test was later meta-analyzed by Payne et al.&nbsp;(1994), who gathered 44 effect sizes, of which 14 effects did not show significant differences between misled and non-misled participants. However, when assessing the effect size jointly, the mean recognition level was significantly lower in the misled conditions compared to the control conditions. Ultimately, Payne et al found that the misinformation effect was observed even with the modified test paradigm, which contradicted the theory and predictions developed by McCloskey and Zaragoza (1985).</p>
<p>Theories of the misinformation effect that do not entail memory impairment (failure in retrieval) such as the one provided by McCloskey and Zaragoza (1985) are referred to as “Strategic Effects” accounts (Ayers &amp; Reder., 1998). These theories cover a multitude of potential causes, ranging from guessing strategies to social influences with regards to individuals’ performance on recall tests.</p>
<p>One theory of the misinformation effect that accounts for both memory impairment and effective strategies, is the source monitoring framework (Lindsay &amp; Johnson, 1989a; Johnson et al., 1993). Source monitoring explains the misinformation effect through multiple traces much like the blocking account, but rather than attributing incorrect recall to recency of information, they do it through source misattribution. That is, the two memory traces become muddled, and thus recall of the correct source becomes difficult. The source monitoring account also explains the misinformation effect as a function of memory decay and encoding failure, incorporating much of the findings from the Strategic Effects literature.</p>
</section>
<section id="moderating-factors" class="level2">
<h2 class="anchored" data-anchor-id="moderating-factors">Moderating Factors</h2>
<p>The research on potential moderators of the misinformation effect is extensive and provides information about both internal and external factors that influence the magnitude of the effect. By virtue of the three-phase paradigm, many manipulations can be introduced into the experimental design. This has led to a literature that is highly varied in experimental manipulations but follows the same underlying design structure.</p>
<section id="design-moderators" class="level3">
<h3 class="anchored" data-anchor-id="design-moderators">Design Moderators</h3>
<p>There are many potential ways to apply and combine moderators to the three-phase design. We have covered the most prominent moderating design adaptions below, but many other design manipulations are present within the literature such as exposure credibility (e.g., Echterhoff et al., 2005), stress, (e.g., Zoladz et al., 2017), valence (e.g., Paz-Alonso &amp; Goodman, 2016) or hypnosis (e.g., Wagstaff et al., 2008). A diagram of the three-phase design along with potential design moderators is viewable in Figure 1.</p>
<section id="figure-1" class="level4">
<h4 class="anchored" data-anchor-id="figure-1">Figure 1</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/design_flow_chart.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Note: Circles indicate an example of possible moderating factors at each stage of the experimental paradigm. Exposure materials refer to different measures of manipulating misinformation</figcaption><p></p>
</figure>
</div>
</section>
<section id="retrieval-enhanced-suggestibility-res" class="level4">
<h4 class="anchored" data-anchor-id="retrieval-enhanced-suggestibility-res">Retrieval Enhanced Suggestibility (RES)</h4>
<p>Introducing an initial test prior to the exposure of misinformation, was first hypothesized to mitigate the misinformation effect (Chan et al., 2009). The hypothesis was based on traditional verbal-learning research and testing-effect research. In traditional verbal-learning research, it was found that participants’ recollection of pictures was improved after repeated recall (Erdelyi &amp; Vecker, 1974). In testing-effect research, results indicated that participants had an improved recall performance if they had an initial test prior to the final test (Chan et al., 2006). Surprisingly, the results from Chan et al.&nbsp;(2009) were the opposite of what was previously expected. Their findings suggested that an initial test enhanced participants’ susceptibility to misinformation, which led to the development of the retrieval enhanced suggestibility (RES) literature. Chan et al.&nbsp;(2009) speculated that this finding could be explained in two ways: 1) immediate recall enhances the learning of subsequent misinformation, and 2) immediate recall enhances susceptibility of interference in recently recalled items.</p>
<p>Thomas et al., (2010) found that the increased susceptibility to misinformation after an initial recall test, occurred if two conditions were met: 1) the participants based their answer on what information was most accessible in memory, and 2) the participants were not actively monitoring the source of the retrieved information. Misled participants in the initial test condition performed significantly worse on the final test compared to misled participants in the control condition. However, this effect disappeared when participants in the repeated recall condition were warned that some information may have been inaccurate in the post-event information phase.</p>
</section>
<section id="warnings" class="level4">
<h4 class="anchored" data-anchor-id="warnings">Warnings</h4>
<p>There are two types of warnings in the misinformation literature: the pre-warnings, and the post-warnings. Pre-warnings refers to warnings presented prior to the post-event information, indicating that misinformation may be present in the upcoming phase. Post-warnings are introduced after the post-event information, and refers to warnings indicating that the previously encountered information may have contained misleading information. In a meta-analysis of the post-warning literature, Blank and Launey (2014) found a significant effect of post-warnings. Their findings indicated that certain warnings prove to be more effective than others. Additionally, they observed that the decrease in memory distortion came at a minor expense to memory accuracy under control conditions.Pre-warnings are not as prevalent within the literature as post-warnings, therefore no synthesized evidence for the effect of pre-warnings is available. However, one of the earliest studies on the topic (Winkelspecht &amp; Mowrer, 1999) indicated that pre-warnings of post-event misinformation could not produce a significant difference in final recall accuracy.</p>
</section>
<section id="retention-intervals" class="level4">
<h4 class="anchored" data-anchor-id="retention-intervals">Retention Intervals</h4>
<p>The time intervals between each phase in the misinformation effect paradigm have been extensively researched. These studies manipulate the retention interval between the original event and the post-event information, or the retention interval between the post-event information and the final test. However, the study designs are usually confounded with other variables, making it difficult to discern a pure effect of timing. Loftus et al.&nbsp;(1978), discovered that when misleading information was introduced immediately before the final test compared to just after the original event, the misinformation effect was larger. Yet, the interpretation of these results is rather unclear. The effect of each retention interval cannot be distinguished from one another; therefore, no conclusion may be drawn on whether it is the longer delay between the original event and the PEI-phase, or the short delay between the PEI-phase and the test phase, that has the largest influence on suggestibility. Moreover, Higham (1998) failed to replicate these timing differences, as no significant differences were found in the misinformation effect between the two different retention intervals. Although it is uncertain which retention interval has a greater influence, the literature indicates that longer retention intervals generally produce a larger effect (Payne et al., 1994).</p>
</section>
<section id="test-type-and-control-accuracy" class="level4">
<h4 class="anchored" data-anchor-id="test-type-and-control-accuracy">Test-Type and Control Accuracy</h4>
<p>The literature presents various measures for assessing memory accuracy. Many different types of tests have been developed to obtain a deeper understanding about the nature of the misinformation effect. In the modified recall test developed by McClosky and Zaragoza (1985), participants are presented with a two-choice forced recognition test containing the accurate item and a novel item. As mentioned before, this testing procedure was predicted to not generate a misinformation effect but was refuted by Payne et al.&nbsp;(1994). However, differences in the magnitude of the misinformation effect between the modified test procedure and the standard test procedure have not yet been estimated using modern statistical tools, calling for further exploration.</p>
<p>Recognition tests where the subjects are forced to distinguish the correct item from other misleading and/or novel items has been frequently utilized to measure memory accuracy within the paradigm (e.g., Dodd &amp; Bradshaw, 1980; Mojtahedi et al., 2017). Another common measure is the cued recall test, where the subjects report their recollection by the aid of cues (e.g., Powell, 1999; Quas et al., 2007). Free recall is also a frequently used recall measure, where the participants are allowed to freely recall the event without the use of any special aids or cues (e.g., Marche &amp; Howe, 1995; Peterson, 2004). A popular alternative to the more traditional tests is the source monitoring test (e.g., Lane et al., 2001; Pereverseff et al., 2020). The objective of the test is to correctly identify the source of a particular piece of information. Within the misinformation paradigm this works by asking the participants to determine if an item in question was presented in the original event or in the post-event information phase.</p>
<p>Except for the most common recall measures mentioned above, interpolated recall (Belli, 1993) and the modified modified free recall test (MMRF) (Barnes &amp; Underwood, 1959; Belli et al., 1994) has also been utilized. In the interpolated recall test, participants are asked to remember specific items from the original event, such as the brand of a certain item. In the MMFR test, participants are warned that they have been exposed to misleading information. Participants are allowed to report items from the original event and from the post-event information phase to reduce response biases and misinformation susceptibility. While most studies utilize a common cued recall or recognition test, they are not suited for all areas of research; therefore, it is important to establish potential differences between these test types. An additional question posed by Payne et al (1994), is whether a high degree of control accuracy is required for a misinformation effect to occur. That is, if the size of the misinformation effect is dependent on the overall control accuracy of the participants. Their findings suggested that there was a clear linear relationship between control accuracy and the misinformation effect, where higher accuracy led to larger effects.</p>
</section>
</section>
<section id="demographic-moderators" class="level3">
<h3 class="anchored" data-anchor-id="demographic-moderators">Demographic Moderators</h3>
<p>While most moderators connected to the misinformation effect targets design adaptations, some parts of the literature focus on demographic characteristics. These include studies targeting gender differences (e.g., Mojtahedi et al, 2019) and analyses in specific populations such as individuals with alcohol dependence (e.g.&nbsp;Ober &amp; Stillman, 1988) or neurological conditions (e.g.&nbsp;Bowles &amp; Sharman, 2013). In this literature summary, we will only cover age differences, due to it having the largest pool of studies.</p>
<section id="age" class="level4">
<h4 class="anchored" data-anchor-id="age">Age</h4>
<p>Wylie et al.&nbsp;(2014) conducted a meta-analysis on differences in misinformation susceptibility between older and younger adults. The analysis was spurred by inconsistencies they perceived in the literature, such as different definitions of age groups, homogenous samples, and methodological differences. They extracted 39 independent effect sizes consisting of 3,534 participants. Their findings suggested that when the mean age of the older adult samples increased, the effect sizes between the older and younger adults increased as well, indicating that misinformation susceptibility was larger for the older adult samples than the younger adult samples. A follow-up regression analysis on both younger and older samples revealed a significant regression-coefficient for older adults and a non-significant regression-coefficient for younger adults This suggests that the effect of age on misinformation susceptibility follows a non-linear relationship, where it goes from statistically insignificant in younger adults to statistically significant in older adults.</p>
<p>Another way to assess variation in the misinformation effect across ages is through the examination of children. The meta-analysis by Ceci &amp; Bruck (1993) indicates that preschoolers (aged between three and five) are more susceptible to misinformation when compared to older children or adults. Of the 18 studies used in their review, 15 indicated that preschoolers were the most suggestible age group, however, no absolute magnitude of this difference in suggestibility is available.</p>
</section>
</section>
</section>
<section id="the-current-meta-analysis" class="level2">
<h2 class="anchored" data-anchor-id="the-current-meta-analysis">The Current Meta-Analysis</h2>
<p>The aim of our meta-analysis is to put the misinformation effect under scrutiny once more and examine the moderating effects of various variables. The objective is to generate a high-level overview of the misinformation effect literature, with a wish to expand and clarify the current stage of knowledge about the nature of the phenomenon. We seek to replicate previous meta-analytic findings from the literature, such as the effect of the modified test (Payne et al., 1994), post-warnings (Blank &amp; Launay, 2014), and age differences (Ceci &amp; Bruck, 1993; Wylie et al., 2014). Additionally, we wish to identify potential limitations from the literature which may indicate a need for further exploration, and through that provide recommendations for future research. In accordance with our aims, we have formulated hypotheses for directed testing, and exploratory research questions designed to give a wide description of the current state of knowledge. Table 1 provides an overview of the hypotheses and research questions explored in this thesis.</p>
</section>
</section>
<section id="method" class="level1">
<h1>Method</h1>
<section id="open-practices" class="level2">
<h2 class="anchored" data-anchor-id="open-practices">Open Practices</h2>
<p>To assure transparency and aid future replications, we pre-registered our study (https://osf.io/unxc6) and followed the guidelines of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) (Page et al., 2021). In the method section, we have reported the strategies behind the identification, selection, assessment, and integration of the studies extracted for the meta-analysis. Significant decisions and actions taken in the study are also reported below. More details on the pre-registration, data extraction and code are accessible at the Open Science Framework (OSF), https://osf.io/qxptz/.</p>
</section>
<section id="procedure" class="level2">
<h2 class="anchored" data-anchor-id="procedure">Procedure</h2>
<section id="literature-search-procedures" class="level3">
<h3 class="anchored" data-anchor-id="literature-search-procedures">Literature Search Procedures</h3>
<p>The first step in developing the search strategy was to consult with the librarians at the University of Gothenburg with the aim of finding suitable databases to extract studies from. The consultation led to the identification of four databases: PsycInfo, PsychArticles, Criminology Collection and Scopus, and will be described in the following section.</p>
<p><em>PsycInfo.</em> A database containing abstracts of research in psychology produced by the American Psychological Association (APA). <em>PsycArticles.</em> A database produced by the APA containing journal articles in behavioral sciences and associated disciplines. <em>Criminology Collection.</em> This collection is organized by ProQuest and includes two databases: the Criminal Justice Database and the National Criminal Justice Reference Service Abstracts Database. The former provides full-text access to crime research and the latter provides journal articles, publications and reports within criminal justice, law enforcement and associated disciplines. <em>Scopus.</em> The database is produced by Elsevier and contains abstracts of research within the disciplines of health science, life science, physical science and social science.</p>
<p>After the identification of the databases, we proceeded with the first search on the 8th of December 2021. We used ProQuest, a platform for literature search, to scan PsycInfo, PsycArticles and the Criminology Collection databases simultaneously. Scopus was searched independently. No filters or limits were used in the search strategy. The following search terms were used: noft(Post?event information) OR noft(Misinformation effect) OR noft(Post?event misinformation). The literature search yielded 709 articles. We noticed that multiple original articles were absent from the initial search and proceeded to expand the search terms to increase the possibility of finding all relevant records. Additional search terms were added and then utilized in an additional literature search on the 21st of February 2022. The following search terms were used in the final search: “noft(misinformation) OR noft(misbelief<em>) OR noft(”misleading information”) OR noft(”misinformation effect”) OR noft(”eyewitness suggestibility”) OR noft(”misleading question</em>”) OR noft(“post?event misinformation”) OR noft(“leading question*”) OR noft(“post?event information”)“. This yielded 5,267 records, of these 1,362 were duplicates, leaving us with 3,914 unique records.</p>
</section>
<section id="criteria-for-inclusion-and-exclusion-of-studies" class="level3">
<h3 class="anchored" data-anchor-id="criteria-for-inclusion-and-exclusion-of-studies">Criteria for Inclusion and Exclusion of Studies</h3>
<p>Studies had to employ an experimental design with a control condition to be included. The control conditions were allowed to be both between (control group) and within subjects (control items). We only included original studies and did not include republished studies or studies with pre-existing data. In cases where a dissertation and a belonging published version were available, we decided to extract the published version to benefit from the quality assessment of the peer review. Only studies with non-infant human participants were included. While there are studies on the ME using infants (e.g., Boller et al., 1995) and non-human animals (e.g., Schwartz et al., 2004) as subjects, we argue that the results of these studies are not comparable to those using human children and adults. The reason for this is that the experimental design in these studies fails to capture the misinformation phase in the three-phase paradigm. The stimuli in the misinformation phase are supposed to contradict or replace stimuli shown in the PEI phase. If the misinformation stimuli fail to do this, we cannot comfortably assume that it is the misinformation effect we are capturing.</p>
<p>Accordingly, the studies had to utilize the three-phase misinformation effect paradigm to be included. Memory accuracy in the final test had to be measured and clearly reported, that is, a percentage of correct responses to control and misled items had to be described. If only rates of misinformation recall were reported, the record was not extracted. Additionally, for studies reporting memory accuracy as counts (number of items recalled) to be included, they had to provide clear reports of how many possible counts of correct recollection was possible. Otherwise, the differences between misled and control groups could not be transformed to the same scale. No study was excluded based on the presence of moderator variables as long the moderator and the design did not conflict with the three-phase design paradigm. Original events that contained to-be-remembered information from an untraceable source were excluded from the study due to the lack of experimental control. An example of this would be studies that use general knowledge as original events. Collaborative recall events where two or more participants recalled the original event simultaneously were excluded due to not having an individual test phase. Apart from these exceptions, no studies were excluded based on the type of original event, post-event information or final test.</p>
</section>
<section id="screening-procedures" class="level3">
<h3 class="anchored" data-anchor-id="screening-procedures">Screening Procedures</h3>
<p>All records identified from the literature search were uploaded to Rayyan for screening. Rayyan is an online tool for collaborative review research where referential data, including abstracts, can be uploaded and surveyed for inclusion or exclusion in a systematic review or meta-analysis (Mourad et al., 2016). As of 2022, Rayyan provides two automated services powered by artificial intelligence: duplication detection and study inclusion probability ratings. The only automated tool used in the present study was duplication detection. The duplication detector compares abstracts and referential information of all studies and reports a similarity rating in percentages. Records that have high similarity ratings are grouped together and records that are identical are deleted automatically. Before screening any of the abstracts, we looked through the record pairings provided by Rayyan and checked if they were duplicates or if they were unique records. All 1,362 duplicates were removed before the first screening. The first round of screening consisted of reading all abstracts and reviewing their suitability for inclusion in the meta-analysis. Articles that had no connection to the misinformation effect or used the term misinformation in a different setting other than the three-phase paradigm were excluded. Examples of this would be using the word “misinformation” with regards to the spread of false information or referring to suggestibility and misinformation effects but detailing different experimental designs such as the Deese-Roediger-McDermott (DRM) false memory paradigm or the Gudjonsson Suggestibility Scale (GSS) studies. Note however that the inclusion of any other tests did not serve as an exclusion criterion if the three phase post-event misinformation design was followed. Of the 3,914 unique records identified, 550 remained after the first round of screening.</p>
<p>The second stage of the screening consisted of reviewing the articles in their entirety and assessing whether they followed the three-phase design, had sufficient control conditions and used non-infant human participants. In the second screening stage, we also examined whether the studies reported enough information for any meaningful synthesis. That is, if the studies report information such as sample and effect size, or enough information for those measures to be inferred outside of the study. Studies that were lacking in clear and relevant statistical reporting were excluded at this stage. In the first two stages of screening, we were conservative with regards to exclusion, choosing to err on the side of caution and include studies that we were unsure if they should be excluded.</p>
<p>The first and second screening was done by both authors. We divided and processed approximately half of the articles each in the first and second screening round. If no clear decision on whether to include or exclude a record could be made, we conferred with each other and our supervisor. The second screening stage was conducted by the same authors and in the same manner as the first. Of the 550 records included in the second screening, 389 were deemed suitable for extraction.</p>
<p>The third stage of screening involved data extraction. In this stage, studies that were included in the previous stages were re-examined with regards to their statistical reporting. Studies that failed to report enough information were excluded. Data extraction was mainly done by the authors, but assistance from the supervisor was required in the latter stages of extraction. Like in previous stages, if issues concerning the data extraction arose, the issue was discussed to reach a conclusion of whether to exclude the study at hand. In the third stage, five additional studies were identified through examining reference lists, two of which were suitable for extraction.</p>
</section>
<section id="variables-coded-from-each-report" class="level3">
<h3 class="anchored" data-anchor-id="variables-coded-from-each-report">Variables Coded from Each Report</h3>
<p>The main variables we sought information about were a) recall accuracy in proportions, b) mean recall accuracy, c) the standard deviation of mean recall accuracy, and d) sample sizes for both control and treatment conditions. These variables depict the degree to which individuals make correct recollections about the source material on any given test. Depending on how the control condition was formulated, these recall accuracies, means and standard deviations were either from the control group or from the control items in the within-subject designs.</p>
<p>Three different types of control conditions within the misinformation paradigm have been reported in the literature. In the post-event information stage, the participants are either exposed to consistent (accurate) information, neutral (unspecific accurate) information or receive no post-event information at all. In cases where more than one control condition was reported, we sought to extract data from the control condition exposed to consistent information (if present). The reasoning behind this decision is that comparing consistent and misleading PEI is seemingly the most straightforward way of examining item manipulation. Additionally, if the experiment utilized a mixed design, we sought to extract data from within-subjects’ comparisons to benefit from the increase in power.</p>
<p>Recall accuracy in proportions refers to the number of individuals who correctly report one item in the control and misled condition. This measure is a sample aggregate used in experimental designs that only have one misled item, a common practice in early misinformation research (e.g., Loftus, 1975). Mean accuracy depicts the arithmetic mean for each item and may be described as the mean number of correct details in the control conditions and mean number of correctly recalled details in the misled condition. This measure is typically employed in designs utilizing multiple misled details and can thus be aggregated within and across subjects.</p>
</section>
</section>
<section id="data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="data-analysis">Data Analysis</h2>
<section id="statistical-software" class="level3">
<h3 class="anchored" data-anchor-id="statistical-software">Statistical Software</h3>
<p>All data analyses were conducted in R (R Core Team, 2020) using RStudio, an integrated development environment for R (RStudio Team, 2020). The R package metafor (Viechtbauer, 2010), was utilized to calculate effect sizes and estimate meta-analytic models, and ggplot2 (Wickham, 2016) for visualizations.</p>
</section>
<section id="effect-size-estimate" class="level3">
<h3 class="anchored" data-anchor-id="effect-size-estimate">Effect Size Estimate</h3>
<p>We computed the effect size estimate Hedges’ g to assess the magnitude of the misinformation effect. This was done by computing standardized mean differences (SMD) in test accuracy between each study’s experimental condition and control condition. These standardized mean differences were then divided by the means of the standard deviations for the experimental groups and the control groups. If standard deviations were not reported, we used other methods to estimate their value such as inferring them from test-statistics. For studies only reporting proportions, the log odds were calculated and then transformed into standardized mean differences.</p>
</section>
<section id="primary-analysis" class="level3">
<h3 class="anchored" data-anchor-id="primary-analysis">Primary Analysis</h3>
<p>The aim of the primary analysis was to estimate an overall effect size using all studies. A three-level model was employed to account for the interdependence of the extracted effects, using control conditions nested within individual experiments which in turn were nested within the given record (Hedges, 2009). To avoid assumptions of homogeneity across studies that had different units-of-analysis in terms of study material, a three-level mixed effects model was conducted with study material as a random effects variable. Study materials refers to the to-be-remembered event in the first phase of the three-phase paradigm. We also assumed heterogeneity across studies originating from different countries. This led to the a priori decision to update the model with the country of origin as an additional random effects variable.</p>
<p>The goodness-of-fit across models was assessed using the conventional fit indices Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), log likelihood (logLik) and the likelihood-ratio test (LRT). The AIC fit measure provides a method for model selection by estimating prediction error and the quality of each model in relation to the other models (Akaike, 1973). The model with the lowest AIC value is perceived as the best fit and accounts for the biggest amount of variation given the least possible independent variables in the model. AIC provides a penalty term for every added parameter in the model to decrease the risk of overfitting. The formula and function of the BIC is similar to that of the AIC, with the exception that BIC also takes the number of observations into account. This adds a larger penalty term for each parameter in the model compared to the AIC. The model with the lowest BIC value indicates the best model fit (Schwarz, 1978). The “log likelihood” fit index is the natural logarithm of the likelihood, meaning that higher values correspond to a higher likelihood of the model. As supplementation for our fit measures, we also conducted likelihood-ratio tests (LRTs). The LRT serves as a null hypothesis test that compares the fit of nested models against each other. If the likelihood of the models differs by more than what would be expected from sampling error, we conclude that they fit to the data differently and we select the model with the better fit.</p>
<section id="heterogeneity" class="level4">
<h4 class="anchored" data-anchor-id="heterogeneity">Heterogeneity</h4>
<p>After identifying the best fitting model, we assessed the heterogeneity of effect sizes across studies. We computed and inspected the results of Cochran’s Q-test and <span class="math inline">\(I^2\)</span> statistics to examine outcome variability between studies. Cochran’s Q-test, being a chi-squared test on the Q statistic (Cochran, 1954), examines whether a statistically significant heterogeneity is present or not. If the test statistic is large, the null hypothesis is rejected, indicating that some non-zero value of heterogeneity across studies exists at an <span class="math inline">\(a\)</span> &lt;.1. In essence, a rejection of this null hypothesis would indicate that the variation in the observed effect sizes is larger than would be expected by chance, meaning that the effects might not be easily comparable.</p>
<p>The findings were then complemented by calculating the <span class="math inline">\(I^2\)</span> value, which provided an estimate and confidence interval for how much between-study variability in the observed effect sizes could be explained by other factors besides random errors (Higgins &amp; Thomson, 2004; Thorlund et al., 2012). Variance components were computed to quantify the magnitude of heterogeneity, providing a point estimate of variance at different levels of the model.</p>
</section>
<section id="bias-assessment" class="level4">
<h4 class="anchored" data-anchor-id="bias-assessment">Bias Assessment</h4>
<p>Publication/small sample bias was adjusted using the Egger regression intercept (Egger et al., 1997), displaying a linear regression of the effect sizes’ standard errors, weighed by their inverse variance through funnel plot asymmetry. The intercept should be zero given that no publication bias is present. To adjust for the correlation between effect sizes and standard errors, Egger’s test was conditioned on the precision-effect estimate with standard error, commonly abbreviated PET-PEESE (Stanley &amp; Doucouliagos, 2014). In addition, we analyzed the distribution of p-values to detect problematic research practices, and to assess publication biases within the extracted literature.</p>
</section>
</section>
<section id="secondary-analyses" class="level3">
<h3 class="anchored" data-anchor-id="secondary-analyses">Secondary Analyses</h3>
<p>The secondary analyses consisted of multiple subgroup moderator analyses and meta-regression models assessing third variables along with indicators for the study population and demographics. The purpose of the moderator analyses was twofold, we wanted to address potential heterogeneity in our primary analyses and examine the magnitude of the moderators´ association with the misinformation effect. We conducted separate analyses for each subgroup instead of fitting a single meta-regression model on the entire sample, due to the large sample size, missing data and variability between study designs (Higgins &amp; Thomson, 2004).</p>
<section id="experimental-design-moderators" class="level4">
<h4 class="anchored" data-anchor-id="experimental-design-moderators">Experimental Design Moderators</h4>
<p>The first assortment of analyses aimed to assess the moderating effects of different experimental designs. These analyses targeted potential effects related to manipulations done by the researcher(s) that alters the experimental design.</p>
</section>
<section id="the-moderating-effect-of-initial-tests" class="level4">
<h4 class="anchored" data-anchor-id="the-moderating-effect-of-initial-tests">The Moderating Effect of Initial Tests</h4>
<p>In this moderator analysis, we examined the effect of initial testing prior to PEI. The variable was a count variable indicating the number of initial tests. This analysis examined the effect of retrieval enhanced suggestibility (RES).</p>
</section>
<section id="the-moderating-effect-of-warnings." class="level4">
<h4 class="anchored" data-anchor-id="the-moderating-effect-of-warnings.">The Moderating Effect of Warnings.</h4>
<p>In this moderator analysis, we examined the effect of warnings. We analyzed the effect of receiving a warning before being exposed to PEI (pre-warnings), and the effect of receiving a warning after the PEI (post-warnings). The meta-analysis by Blank &amp; Launey (2014) found that a post-warning reduced the misinformation effect to 43% of its’ original size by comparing the log odds of warned and non-warned participants. This reflects a standardized mean difference of g = -0.267 for the difference between warned and non-warned participants. This represents a small yet meaningful reduction in final test accuracy for the non-warned participants. We aimed to replicate this finding and expand it by incorporating the effect of pre-warnings.</p>
</section>
<section id="the-moderating-effect-of-test-type" class="level4">
<h4 class="anchored" data-anchor-id="the-moderating-effect-of-test-type">The Moderating Effect of Test-Type</h4>
<p>In this moderator analysis, we examined the effect of test-type. This variable describes the nature of the final test procedure. We compared differences between all reported test-types. The main difference between the various tests lie in how the questions on the test are phrased and what kind of responses are prompted from the participants.</p>
</section>
<section id="the-moderating-effect-of-study-modality" class="level4">
<h4 class="anchored" data-anchor-id="the-moderating-effect-of-study-modality">The Moderating Effect of Study Modality</h4>
<p>In this moderator analysis we examined the potential effect of study modality. We examined eventual differences in the misinformation effect depending on the context in which the studies were carried out. We compared the effect across field, laboratory, and online studies.</p>
</section>
<section id="the-moderating-effect-of-retention-intervals" class="level4">
<h4 class="anchored" data-anchor-id="the-moderating-effect-of-retention-intervals">The Moderating Effect of Retention Intervals</h4>
<p>In this moderator analysis we examined how retention intervals between the phases in the paradigm affected the magnitude of the effect. Firstly, we examined the effect of the time between the original event and receiving the post-event information. Secondly, we examined the effect of the time between receiving the post-event information and recalling the original event. During data extraction, time was measured in hours. However, some studies utilized retention intervals expanding over several months. Therefore, we chose to convert time into days during the analysis stage to facilitate the interpretation of the results.</p>
</section>
</section>
<section id="demographic-moderators-1" class="level3">
<h3 class="anchored" data-anchor-id="demographic-moderators-1">Demographic Moderators</h3>
<p>The second assortment of analyses aimed to assess the moderating effects of participant demographics. These analyses targeted potential differences due to the characteristics of the sample studied.</p>
<section id="the-moderating-effect-of-age" class="level4">
<h4 class="anchored" data-anchor-id="the-moderating-effect-of-age">The Moderating Effect of Age</h4>
<p>With this moderator analysis, we examined age both as a continuous and categorical variable. The age variable was described as the mean age of the sample. To address the question of linearity in the effect, we fitted age as both linear and quadradic terms. We also aimed to replicate the results of the meta-analyses conducted by Ceci and Bruck (1993), and Wylie et al., (2014).</p>
<p>The meta-analysis by Wylie et al.&nbsp;(2014) estimated the random effects weighted mean difference between younger and older adults to be rm = .35 (95% CI [.22, .47]). Indicating a moderate effect of age in terms of older adults being more susceptible to misinformation than younger adults. A follow-up regression analysis on both younger and older samples confirmed this difference by revealing a significant regression-coefficient (β = .06) for older adults and a non-significant regression-coefficient in younger adults (β = -.02). The meta-analysis by Ceci and Bruck (1993) examined differences between children and adults. However, no point estimates for prediction were reported. Our replication strived to compare four discrete age groups, ranging from children to older adults (younger children, older children, adults and older adults).</p>
</section>
<section id="the-moderating-effect-of-gender" class="level4">
<h4 class="anchored" data-anchor-id="the-moderating-effect-of-gender">The Moderating Effect of Gender</h4>
<p>This moderator analysis examined the effect of gender. The aim was to examine if the proportion of female participants in the study sample impacted the magnitude of the misinformation effect.</p>
</section>
</section>
<section id="moderating-effects-of-study-characteristics." class="level3">
<h3 class="anchored" data-anchor-id="moderating-effects-of-study-characteristics.">Moderating Effects of Study Characteristics.</h3>
<p>The third and final assortment of analyses assessed potentially moderating effects due to the characteristics of the studies included in the dataset. These analyses targeted moderating effects not specifically manipulated by the researchers, but effects due to the nature of the study.</p>
<section id="the-moderating-effect-of-control-test-accuracy." class="level4">
<h4 class="anchored" data-anchor-id="the-moderating-effect-of-control-test-accuracy.">The Moderating Effect of Control Test Accuracy.</h4>
<p>In this moderator analysis, we analyzed the effect of control test accuracy on the magnitude of the misinformation effect. This decision was influenced by the findings of Payne et al (1994), which found a significant relationship between control test accuracy and the misinformation effect (r = .54, p &lt; .001). Noteworthy, the sample used by Payne et al was limited to 44 studies, which exhibited a relatively small range of accuracies, varying from around 60% to 90%. This brings up the question of whether the moderating effect of control accuracy follows a linear or a non-linear relationship.</p>
</section>
<section id="the-moderating-effect-of-publication-year" class="level4">
<h4 class="anchored" data-anchor-id="the-moderating-effect-of-publication-year">The Moderating Effect of Publication Year</h4>
<p>In this moderator analysis, we examined the potential effect of study publication year. We examined how the misinformation effect varied throughout the history of the literature and if the general effect was stable across time.</p>
</section>
</section>
<section id="sensitivity-analysis" class="level3">
<h3 class="anchored" data-anchor-id="sensitivity-analysis">Sensitivity Analysis</h3>
<p>To check the robustness of our models and to assess the impact of key decisions in the analysis, various sensitivity analyses were conducted. We were particularly interested in identifying influential cases that may have affected the effect estimate. To account for possible influential cases, Cook’s distance was calculated (Viechtbauer &amp; Cheung, 2010), and visual inspections of the distribution of effects were carried out. Cook’s distance provides an estimate of how much a specific data point negatively affects the regression model. The leverage and the residual of each data point is taken into consideration, yielding a summary of how much the model alternates when an influential data point is removed. Ideally, more than just one influence measure would be reviewed, but given the complexity of our model and the computational intensity of the measures, an extensive influence analysis in this case was not feasible. In addition to calculating Cooks’ distance, a funnel plot was inspected to identify possible extreme values/outliers. The subgroup sensitivity analyses consisted of constraining variables to certain values and assessing whether the model estimates were consistent across these constraints. For example, in the analysis of the effect of PEI retention intervals, we refitted the model across various intervals to examine changes in the effect.</p>
</section>
<section id="other-extracted-variables" class="level3">
<h3 class="anchored" data-anchor-id="other-extracted-variables">Other Extracted Variables</h3>
<p>Prior to extracting all data, a coding draft was constructed which contained all potential moderating variables that we identified from the literature. This included factors such as neurological conditions of the participants, exposure credibility, event valence and alcohol consumption. After the data extraction, we noticed that these variables were not as commonly assessed as first supposed. The small number of observations connected to these variables led to the conclusion that no meaningful moderator analysis could be carried out.</p>
</section>
<section id="ethics" class="level3">
<h3 class="anchored" data-anchor-id="ethics">Ethics</h3>
<p>To ensure adequate research ethics in the current meta-analysis, we followed the ethical guidelines developed by the Swedish Research Council (2017). We have reported the study procedure, methodology, results, and our interpretation of them in an accurate and transparent fashion. Prior to the data analysis, we have consciously and transparently reported our starting point with the aid of preregistration. We have conveyed no conflict of interest or other ties. We are not in a position for financial gain or loss, nor affected in any other way that may bias our results. We have secured a satisfactory order in our research by documenting and archiving the research process in a publicly Open Science Framework (OSF) available at: https://osf.io/qxptz/. The data used for analysis has been extracted from previous reports, implying that we have no ethical responsibility regarding how the experiments have been conducted.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>