---
title: "results_report"
format: html
editor: visual
execute: 
  echo: false
---

```{r prepearing and loading data, message=FALSE, warning=FALSE}
library(tidyverse)
library(metafor)
library(cowplot)
library(MASS)
library(here)
library(future)

# Load data 
data_es <- read_csv('misinformation_clean_data.csv')

# Center moderator variables

data_es <- data_es %>% 
  mutate(
    gender_female_prop          = gender_female_prop - .50,
    publication_year            = publication_year - mean(publication_year, na.rm = TRUE),
    control_acc                 = case_when(
      !is.na(total_accuracy_control_mean) ~ total_accuracy_control_mean,
      is.na(total_accuracy_control_mean) ~ total_accuracy_control_prop
    ),
    control_acc = control_acc - mean(control_acc, na.rm = TRUE),
  )

# Random effect missingness

data_es$event_materials[is.na(data_es$event_materials)] <- "missing"
data_es$country[is.na(data_es$country)]                 <- "missing"
data_es$control_type[is.na(data_es$control_type)]       <- "missing"
data_es$modality[is.na(data_es$modality)]               <- "missing"
data_es$population[is.na(data_es$population)]           <- "missing"
data_es$test_type[is.na(data_es$test_type)]             <- "missing"
data_es$test_medium[is.na(data_es$test_medium)]         <- "missing"
data_es$exposure_medium[is.na(data_es$exposure_medium)] <- "missing"
```

Fitting models

```{r}
# Fitting an initial model only accounting for dependencies
if(!file.exists("meta_initial.rds")){
meta_initial <- rma.mv(yi      = yi, 
                       V       = vi,
                       random  = list(
                         ~1|id_record/id_study/id_control),
                       data    = data_es,
                       method  = "REML", 
                       control = list(
                         iter.max  = 1000,
                         rel.tol   = 1e-8))

meta_initial <- readRDS('meta_initial.rds')
}else{
  meta_initial <- readRDS('meta_initial.rds')
}

# Fitting a full moderation model
if(!file.exists('meta_full.rds')){
meta_full <- rma.mv(yi      = yi, 
                    V       = vi,
                    random  = list(
                             ~1|id_record/id_study/id_control, 
                             ~1|event_materials,
                             ~1|country,
                             ~1|control_type,
                             ~1|modality,
                             ~1|population,
                             ~1|test_type,
                             ~1|test_medium, 
                             ~1|exposure_medium),
                    mods   = 
                            ~ postevent_retention_interval
                            + postexposure_retention_interval
                            + preevent_warning
                            + postevent_warning
                            + postexposure_warning
                            + postevent_recall
                            + postexposure_recall
                            + publication_year
                            + preregistered
                            + control_acc,
                          data    = data_es,
                          method  = "REML", 
                          control = list(
                             iter.max  = 1000,
                             rel.tol   = 1e-8))
saveRDS(meta_full, 'meta_full.rds')
}else{
  meta_full <- readRDS('meta_full.rds')
}
```

Interpreting the initial model.

```{r}
summary(meta_initial)
# Getting the I^2
get_i2 <- function(model){
  tau2_sum <- sum(model$sigma2)
  
  i2_record  <- model$sigma2[1] / tau2_sum * 100
  i2_study   <- model$sigma2[2] / tau2_sum * 100
  i2_control <- model$sigma2[3] / tau2_sum * 100
  
  out <- round(c(i2_record, i2_study, i2_control), 4) 
  names(out) <- c('record', 'study', 'control')
  return(out)
}
get_i2(meta_initial)
```

The model shows that we have substantial heterogeneity across our effects. The $\tau^2$ between records is .28 and the standard deviation of the true effects is .53. The average misinformation effect is .72 with a standard deviation of .03 and a 95% CI(.65\|.8). This illustrates that we have a large mean effect of misinformation, but that this effect has a high level of variation across records. Our naive estimation is that misinformation effects fall under a normal distribution of $N(.72, .28)$ as illustrated in the graph below. We also find a high level of variability within studies attributable to the individuals compared in the effects where different control groups.

```{r}
x <- seq(-3, 3, length.out = 1000)
dens <- dnorm(x, .72, .28)
null <- dnorm(x, 0, 1)

ggplot() +
  geom_line(aes(x = x, y = dens, col = "blue")) +
  geom_line(aes(x = x, y = null, col = "red")) +
  labs(title = "Theoretical Normal Distribution",
       x = "Hedges' G",
       y = "Density")+
  theme_minimal()


ggplot() +
  geom_line(aes(x = x, y = dens, color = "Observed")) +  
  geom_line(aes(x = x, y = null, color = "Null")) +  
  labs(title = "Theoretical Effect Distribution",
       x = "Hedges' G",
       y = "Density") +
  scale_color_manual(name = "Distribution",  
                     values = c("Observed" = "blue",
                                "Null" = "red")) +  
  theme_minimal()
```

Additionally, we a middling level of relative heterogeneity as measured by $I^2$. On the level of the record 60% of the observed variation cannot be attributed to random sampling, on the study level this percentage is 0 and within the control groups this is 41%.

Running the model with robust variance estimation

```{r}
robust.rma.mv(meta_initial, cluster = id_study)
```

The robust variance estimation does not have any substantial impact on the model results as can be seen in the point estimates and the confidence intervals.

Interpreting full model

```{r}
summary(meta_full)
ranef.rma.mv(meta_full)
```

Let's first look at the random effects and the heterogeneity estimates. We observe a large amount of heterogeneity in both moderating random effects and overall between study heterogeneity. Like in the initial analysis we see a between record heterogeneity of $\tau^2 = .21$ with an ($I^2 = 37$%), no heterogeneity attributable across studies within the same record, and then substantial heterogeneity attributable to the control group used within samples nested in studies at $\tau^2=.15$ with an ($I^2 = 37$%).We also see substantial heterogeneity across event materials($\tau^2 = .06$), the control type used($\tau^2 = .05$), recollection test used($\tau^2 = .05$), and test medium($\tau^2 = .04$).

The average misinformation effect remains unchanged from the initial model with an average effect of $g=.69$ with a 95% confidence interval of CI(0.3193, 1.0644). Moderating effects in the random effects variables can be found in the recognition test type variable where use of the interpolated test and the modified test. The interpolated test generally produces larger than average effects when adjusted for the variables included in the model with a $\beta=.3, SE = .14$. As expected, the modified test significantly reduces the effect size with a $\beta=-.4, SE = .1$. Outside of these effects, no individual level of the moderating variables seems to significantly impact the average effect size.

We find a fixed effect of the post exposure retention interval, post-exposure warnings, and control accuracy. An increase in the retention interval seems to reduce the effect, by our scale of time intervals we estimate the average effect to decrease by .0002 for every hour after exposure or roughly .034 every week. The precision of this effect is highly dubious but the passage of time between exposure and recall seems to reduce the effect. Post-exposure warnings reduce the misinformation effect by .26 with a 95% CI(-0.37 ,-0.17). Interestingly, the mean centred control accuracy had by far the largest impact on the size of the average misinformation effect with an estimate of 1.95 with a 95% CI(1.78, 2.13). This means that for each percentage point increase in control accuracy from the average accuracy the expected average misinformation effect incenses by .02. This indicates that how large the misinformation is dependent on how easy the events in the materials are to recall, with easier material producing larger effects. This also explains why little variation was found in some of the moderator variables.

# Publication bias

```{r}
if (file.exists('meta_full_pet.rds'){
meta_full_pet <- rma.mv(yi      = yi, 
                        V       = vi,
                        random  = list(
                             ~1|id_record/id_study/id_control, 
                             ~1|event_materials,
                             ~1|country,
                             ~1|control_type,
                             ~1|modality,
                             ~1|population,
                             ~1|test_type,
                             ~1|test_medium, 
                             ~1|exposure_medium),
                    mods   = 
                            ~ postevent_retention_interval
                            + postexposure_retention_interval
                            + preevent_warning
                            + postevent_warning
                            + postexposure_warning
                            + postevent_recall
                            + postexposure_recall
                            + publication_year
                            + preregistered
                            + control_acc
                            + I(vi^2),
                          data    = data_es,
                          method  = "REML", 
                          control = list(
                             iter.max  = 1000,
                             rel.tol   = 1e-8))

saveRDS(meta_full_pet, 'meta_full_pet.rds')
}else{
  meta_full_pet <- readRDS('meta_full_pet.rds')
}

if(!file.exists('meta_full_peese.rds')){
meta_full_peese <- rma.mv(yi      = yi, 
                          V       = vi,
                          random  = list(
                             ~1|id_record/id_study/id_control, 
                             ~1|event_materials,
                             ~1|country,
                             ~1|control_type,
                             ~1|modality,
                             ~1|population,
                             ~1|test_type,
                             ~1|test_medium, 
                             ~1|exposure_medium),
                    mods   = 
                            ~ postevent_retention_interval
                            + postexposure_retention_interval
                            + preevent_warning
                            + postevent_warning
                            + postexposure_warning
                            + postevent_recall
                            + postexposure_recall
                            + publication_year
                            + preregistered
                            + control_acc
                            + I(vi),
                          data    = data_es,
                          method  = "REML", 
                          control = list(
                             iter.max  = 1000,
                             rel.tol   = 1e-8))
saveRDS(meta_full_peese, 'meta_full_peese.rds')
}else{
  meta_full_peese <- readRDS('meta_full_peese.rds')
}


```

#Interpreting pet-peese

```{r}
summary(meta_full_pet)
summary(meta_full_peese)
```

The pet is significant but also reproduces the effects found in the main models. The peese is also significant but significantly reduces the estimated effect size. This suggests that the presence of publication bias inflates the effect to the extant that the main effect is non-significant. Whether or not this is believable is another question. Personally I do not think this estimate is valid since the effect is clearly established, but that does not mean that our estimate is not inflated. Other models with accounts for dependency differently might be useful here since they give more alternatives on how to account for publication bias such as selection models - but for this we need to fit a normal meta-analysis.
